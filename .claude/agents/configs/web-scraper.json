{
  "agentName": "web-scraper",
  "description": "Use PROACTIVELY for web scraping and content extraction when external data is needed. MUST BE USED with Firecrawl MCP for structured data extraction, multi-page crawling, and competitor data gathering.",
  "model": "claude-3-5-haiku-20241022",
  "mcpServers": {
    "firecrawl-mcp": {
      "command": "npx",
      "args": ["-y", "@firecrawl/mcp-server-firecrawl"],
      "env": {
        "FIRECRAWL_API_KEY": "${FIRECRAWL_API_KEY}"
      }
    }
  },
  "capabilities": [
    "Web page scraping",
    "Content extraction from URLs",
    "Multi-page crawling",
    "Structured data extraction",
    "Website metadata retrieval",
    "Competitor data gathering"
  ],
  "response_format": {
    "type": "structured_data",
    "max_tokens": 500,
    "preferred_formats": ["JSON", "CSV", "Markdown table"],
    "include": [
      "extracted_data",
      "source_urls",
      "extraction_timestamp",
      "data_quality_notes"
    ],
    "exclude": [
      "raw_html",
      "verbose_logs"
    ]
  },
  "routing_triggers": [
    "scrape",
    "crawl",
    "extract from web",
    "fetch URL",
    "web content",
    "website data",
    "competitor data",
    "web research"
  ],
  "special_instructions": [
    "Return data in structured format (JSON/CSV/Markdown)",
    "Include source URLs and timestamps",
    "Handle rate limits gracefully",
    "Validate extracted data quality",
    "Use pagination for large datasets",
    "Respect robots.txt and rate limits"
  ],
  "artifacts": {
    "enabled": true,
    "scratchpad": false,
    "auto_summary": true,
    "detail_threshold": 100,
    "instructions": "Store large scraped datasets in artifacts. Return concise summaries with data statistics and quality metrics."
  },
  "workflow": {
    "step_1_single_page": "Use FIRECRAWL_SCRAPE_EXTRACT_DATA_LLM to scrape single URLs for quick data extraction",
    "step_2_multi_page": "Use FIRECRAWL_CRAWL_URLS to start crawl jobs for multi-page website scraping",
    "step_3_extract": "Use FIRECRAWL_EXTRACT to extract structured data from scraped pages",
    "step_4_status": "Use FIRECRAWL_CRAWL_JOB_STATUS to monitor crawl progress for large jobs",
    "step_5_format": "Format extracted data as JSON, CSV, or Markdown tables",
    "step_6_validate": "Validate data quality and include source URLs and timestamps"
  },
  "mcp_tools_available": [
    "FIRECRAWL_SCRAPE_EXTRACT_DATA_LLM",
    "FIRECRAWL_CRAWL_URLS",
    "FIRECRAWL_EXTRACT",
    "FIRECRAWL_CRAWL_JOB_STATUS",
    "FIRECRAWL_CANCEL_CRAWL_JOB"
  ]
}
